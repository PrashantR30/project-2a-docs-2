{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mirantis Project 0x2A Docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Mirantis Project 0x2A and is focused and developing a consistent way to deploy  and manage Kubernetes clusters at scale. More information can be found here.</p> <p>Project 0x2A was created to be a repeatable and secure way to leverage the existing Kubernetes ecosystem (e.g. Cluster API) while being able to provide for the range of unique use cases that exist within enterprise IT environments. </p>"},{"location":"#main-premise","title":"Main Premise","text":"<p>0x2A is built around the creation of a set of standardised templates that enable  easy, repeatable cluster deployments and life cycle management. </p> <p>The main components of 0x2A include:</p> <ul> <li> <p>Hybrid Multi Cluster Management (HMC)</p> <p>Deployment and lifecycle managment of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>Cluster State Management (SMC)</p> <p>Installation and lifecycle management of beach-head services, policy, Kubernetes api configurations and more.</p> </li> <li> <p>Observability (OBS)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>HMC leverages the Cluster API provider ecosystem, the following providers have  had templates created and validated, and more are in the works.</p> <ul> <li>AWS</li> <li>Azure</li> <li>Vsphere</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n</code></pre>"},{"location":"dev/","title":"HMC installation for development","text":"<p>Below is the example on how to install HMC for development purposes and create a managed cluster on AWS with k0s for testing. The kind cluster acts as management in this example.</p>"},{"location":"dev/#prerequisites","title":"Prerequisites","text":""},{"location":"dev/#clone-hmc-repository","title":"Clone HMC repository","text":"<pre><code>git clone https://github.com/Mirantis/hmc.git &amp;&amp; cd hmc\n</code></pre>"},{"location":"dev/#install-required-clis","title":"Install required CLIs","text":"<p>Run:</p> <pre><code>make cli-install\n</code></pre>"},{"location":"dev/#aws-provider-setup","title":"AWS Provider Setup","text":"<p>Follow the instruction to configure AWS Provider: AWS Provider Setup</p>"},{"location":"dev/#azure-provider-setup","title":"Azure Provider Setup","text":"<p>Follow the instruction on how to configure Azure Provider.</p> <p>Additionally to deploy dev cluster on Azure the following env variables should be set before running deployment:</p> <ul> <li><code>AZURE_SUBSCRIPTION_ID</code> - Subscription ID</li> <li><code>AZURE_TENANT_ID</code> - Service principal tenant ID</li> <li><code>AZURE_CLIENT_ID</code> - Service principal App ID</li> <li><code>AZURE_CLIENT_SECRET</code> - Service principal password</li> </ul> <p>More detailed description of these parameters can be found here.</p>"},{"location":"dev/#vsphere-provider-setup","title":"vSphere Provider Setup","text":"<p>Follow the instruction on how to configure vSphere Provider.</p> <p>To properly deploy dev cluster you need to have the following variables set:</p> <ul> <li><code>VSPHERE_USER</code></li> <li><code>VSPHERE_PASSWORD</code></li> <li><code>VSPHERE_SERVER</code></li> <li><code>VSPHERE_THUMBPRINT</code></li> <li><code>VSPHERE_DATACENTER</code></li> <li><code>VSPHERE_DATASTORE</code></li> <li><code>VSPHERE_RESOURCEPOOL</code></li> <li><code>VSPHERE_FOLDER</code></li> <li><code>VSPHERE_CONTROL_PLANE_ENDPOINT</code></li> <li><code>VSPHERE_VM_TEMPLATE</code></li> <li><code>VSPHERE_NETWORK</code></li> <li><code>VSPHERE_SSH_KEY</code></li> </ul> <p>Naming of the variables duplicates parameters in <code>ManagementCluster</code>. To get full explanation for each parameter visit vSphere cluster parameters and vSphere machine parameters.</p>"},{"location":"dev/#deploy-hmc","title":"Deploy HMC","text":"<p>Default provider which will be used to deploy cluster is AWS, if you want to use another provider change <code>DEV_PROVIDER</code> variable with the name of provider before running make (e.g. <code>export DEV_PROVIDER=azure</code>).</p> <ol> <li> <p>Configure your cluster parameters in provider specific file    (for example <code>config/dev/aws-managedcluster.yaml</code> in case of AWS):</p> <ul> <li>Configure the <code>name</code> of the ManagedCluster</li> <li>Change instance type or size for control plane and worker machines</li> <li>Specify the number of control plane and worker machines, etc</li> </ul> </li> <li> <p>Run <code>make dev-apply</code> to deploy and configure management cluster.</p> </li> <li> <p>Wait a couple of minutes for management components to be up and running.</p> </li> <li> <p>Apply credentials for your provider by executing <code>make dev-creds-apply</code>.</p> </li> <li> <p>Run <code>make dev-mcluster-apply</code> to deploy managed cluster on provider of your    choice with default configuration.</p> </li> <li> <p>Wait for infrastructure to be provisioned and the cluster to be deployed. You    may watch the process with the <code>./bin/clusterctl describe</code> command. Example:</p> </li> </ol> <pre><code>export KUBECONFIG=~/.kube/config\n\n./bin/clusterctl describe cluster &lt;managedcluster-name&gt; -n hmc-system --show-conditions all\n</code></pre> <p>Note</p> <p> If you encounter any errors in the output of <code>clusterctl describe cluster</code> inspect the logs of the <code>capa-controller-manager</code> with: <pre><code>kubectl logs -n hmc-system deploy/capa-controller-manager\n</code></pre> This may help identify any potential issues with deployment of the AWS infrastructure.</p> <ol> <li>Retrieve the <code>kubeconfig</code> of your managed cluster:</li> </ol> <pre><code>kubectl --kubeconfig ~/.kube/config get secret -n hmc-system &lt;managedcluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre>"},{"location":"dev/#running-e2e-tests-locally","title":"Running E2E tests locally","text":"<p>E2E tests can be ran locally via the <code>make test-e2e</code> target.  In order to have CI properly deploy a non-local registry will need to be used and the Helm charts and hmc-controller image will need to exist on the registry, for example, using GHCR:</p> <pre><code>IMG=\"ghcr.io/mirantis/hmc/controller-ci:v0.0.1-179-ga5bdf29\" \\\n    REGISTRY_REPO=\"oci://ghcr.io/mirantis/hmc/charts-ci\" \\\n    make test-e2e\n</code></pre> <p>Optionally, the <code>NO_CLEANUP=1</code> env var can be used to disable <code>After</code> nodes from running within some specs, this will allow users to debug tests by re-running them without the need to wait a while for an infrastructure deployment to occur. For subsequent runs the <code>MANAGED_CLUSTER_NAME=&lt;cluster name&gt;</code> env var should be passed to tell the test what cluster name to use so that it does not try to generate a new name and deploy a new cluster.</p> <p>Tests that run locally use autogenerated names like <code>12345678-e2e-test</code> while tests that run in CI use names such as <code>ci-1234567890-e2e-test</code>.  You can always pass <code>MANAGED_CLUSTER_NAME=</code> from the get-go to customize the name used by the test.</p>"},{"location":"dev/#nuke-created-resources","title":"Nuke created resources","text":"<p>In CI we run <code>make dev-aws-nuke</code> to cleanup test resources, you can do so manually with:</p> <pre><code>CLUSTER_NAME=example-e2e-test make dev-aws-nuke\n</code></pre>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary is a collection of terms related to Project 0x2A. In it we attempt to clarify some of the unique terms and concepts we use or explain more common ones that we feel may need a little clarity in the way we use them. </p>"},{"location":"glossary/#beach-head-services","title":"Beach-head Services","text":"<p>We use the term to refer to those kubernetes services that need to be installed on a kubernetes cluster to make it actually useful, for example: an ingress controller,  CNI and/or CSI. Whilst from the perspective of how they are deployed they are no different from other Kubernetes services we define them as distinct from the apps and services  deployed as part of the applications.</p>"},{"location":"introduction/","title":"Project Overview","text":"<p>Below is a diagram that provides an overview of how Project 0x2A works.</p>"},{"location":"introduction/#architectural-overview","title":"Architectural Overview","text":"<pre><code>---\ntitle: HMC Overview\n---\nerDiagram\n    USER ||--o{ HMC : uses\n    USER ||--o{ Template : assigns\n    Template ||--o{ HMC : \"used by\"\n    HMC ||--o{ CAPI : connects\n    CAPI ||--|{ CAPV : provider\n    CAPI ||--|{ CAPA : provider\n    CAPI ||--|{ CAPZ : provider\n    CAPI ||--|{ K0smotron : Bootstrap\n    K0smotron |o..o| CAPV : uses\n    K0smotron |o..o| CAPA : uses\n    K0smotron |o..o| CAPZ : uses</code></pre>"},{"location":"mk-docs-setup/","title":"Project 0x2A MKdocs Setup","text":""},{"location":"mk-docs-setup/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    stylesheets  # CSS stylesheets to control look and feel\n    assets  # Images and other served material\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"mk-docs-setup/#setting-up-mkdocs-and-dependancies","title":"Setting up MKdocs and dependancies","text":"<ol> <li> <p>Setup python Virtual Environment</p> <p><code>python3  -m venv ./mkdocs</code> <code>source ./mkdocs/bin/activate</code></p> </li> <li> <p>Install MkDocs</p> <p><code>pip install mkdocs</code></p> </li> <li> <p>Install plugins</p> <p><code>pip install mkdocs-mermaid2-plugin</code></p> <p><code>pip install mkdocs-material</code></p> <p><code>pip install markdown-callouts</code></p> </li> </ol>"},{"location":"mk-docs-setup/#run-mkdocs-for-dev","title":"Run MKdocs for dev","text":"<ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> </ul> <p>For full documentation visit mkdocs.org.</p>"},{"location":"mk-docs-setup/#mkdocs-commands","title":"MKdocs Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"aws/cloudformation/","title":"AWS IAM setup","text":"<p>Before launching a cluster on AWS, it's crucial to set up your AWS infrastructure provider:</p> <p>Note</p> <p> Skip steps below if you've already configured IAM policy for your account</p> <ol> <li>In order to use clusterawsadm you must have an administrative user in an AWS account. Once you have that    administrator user you need to set your environment variables:</li> </ol> <pre><code>export AWS_REGION=&lt;aws-region&gt;\nexport AWS_ACCESS_KEY_ID=&lt;admin-user-access-key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;admin-user-secret-access-key&gt;\nexport AWS_SESSION_TOKEN=&lt;session-token&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> <ol> <li>After these are set run this command to create IAM cloud formation stack:</li> </ol> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"aws/cluster-parameters/","title":"AWS cluster parameters","text":""},{"location":"aws/cluster-parameters/#software-prerequisites","title":"Software prerequisites","text":"<ol> <li><code>clusterawsadm</code> CLI installed locally.</li> </ol>"},{"location":"aws/cluster-parameters/#cluster-identity","title":"Cluster Identity","text":"<p>To provide credentials for CAPI AWS provider (CAPA) <code>ClusterIdentity</code> object must be created.</p> <p>AWS provider supports 3 types of <code>ClusterIdentity</code>, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources could be found in CRD Reference.</p> <p>In this example we're using <code>AWSClusterStaticIdentity</code>.</p> <p>To create <code>ClusterIdentity</code> IAM user must be created and assigned with the following roles:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>Follow the IAM setup guide (if not already) to create these roles.</p> <p>Next the following secret should be created with the user's credentials:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cred-secret\n  namespace: hmc-system\ntype: Opaque\nstringData:\n  AccessKeyID: \"AAAEXAMPLE\"\n  SecretAccessKey: \"++AQDEXAMPLE\"\n</code></pre> <p>Note</p> <p> The secret must be created in the same <code>Namespace</code> where CAPA provider is running. In case of Project 2A it's currently <code>hmc-system</code>. Placing secret in any other <code>Namespace</code> will result controller not able to read it.</p> <p>After the <code>Secret</code> was created the <code>AWSClusterStaticIdentity</code> must be created:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cred-secret\n</code></pre> <p>To use these newly created credentials the <code>Credential</code> object must be created. It is described in detail in the credential section.</p>"},{"location":"aws/cluster-parameters/#aws-ami","title":"AWS AMI","text":"<p>By default AMI id will be looked up automatically using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or use AMI ID directly. If both AMI ID and lookup parameters are defined AMI ID will have higher precedence.</p>"},{"location":"aws/cluster-parameters/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup 3 parameters are used:</p> <ul> <li><code>.imageLookup.format</code> - used directly as value for the <code>name</code> filter (see the describe-images filters).</li> <li> <p>Supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - AWS org ID which will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - will be used as value for <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"aws/cluster-parameters/#ami-id","title":"AMI ID","text":"<p>AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"aws/cluster-parameters/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to deploy managed cluster:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"aws/cluster-parameters/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access the nodes using the SSH protocol, several things should be configured:</p> <ul> <li>An SSH key added in the region where you want to deploy the cluster</li> <li>Bastion host is enabled</li> </ul>"},{"location":"aws/cluster-parameters/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ManagedCluster</code> object. The name of the key should then be placed under <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host.</p> <p>To enable bastion you should add <code>.spec.config.bastion.enabled</code> option in the <code>ManagedCluster</code> object to <code>true</code>.</p> <p>Full list of the bastion configuration options could be fould in CAPA docs.</p> <p>The resulting <code>ManagedCluster</code> can look like this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp\n  credential: aws-cred\n  config:\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"aws/hosted-control-plane/","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"aws/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on AWS with HMC installed on it</li> <li>Default storage class configured on the management cluster</li> <li>VPC id for the worker nodes</li> <li>Subnet ID which will be used along with AZ information</li> <li>AMI id which will be used to deploy worker nodes</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"aws/hosted-control-plane/#networking","title":"Networking","text":"<p>The networking resources in AWS which are needed for a managed cluster can be reused with a management cluster.</p> <p>If you deployed your AWS Kubernetes cluster using Cluster API Provider AWS (CAPA) you can obtain all the necessary data with the commands below or use the template found below in the HMC ManagedCluster manifest generation section.</p> <p>If using the <code>aws-standalone-cp</code> template to deploy a hosted cluster it is recommended to use a <code>t3.large</code> or larger instance type as the <code>hmc-controller</code> and other provider controllers will need a large amount of resources to run.</p> <p>VPC ID</p> <pre><code>    kubectl get awscluster &lt;cluster name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre> <p>Subnet ID</p> <pre><code>    kubectl get awscluster &lt;cluster name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre> <p>Availability zone</p> <pre><code>    kubectl get awscluster &lt;cluster name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre> <p>Security group <pre><code>    kubectl get awscluster &lt;cluster name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>AMI id</p> <pre><code>    kubectl get awsmachinetemplate &lt;cluster name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre> <p>If you want to use different VPCs/regions for your management or managed clusters you should setup additional connectivity rules like VPC peering.</p>"},{"location":"aws/hosted-control-plane/#hmc-managedcluster-manifest","title":"HMC ManagedCluster manifest","text":"<p>With all the collected data your <code>ManagedCluster</code> manifest will look similar to this:</p> <pre><code>    apiVersion: hmc.mirantis.com/v1alpha1\n    kind: ManagedCluster\n    metadata:\n      name: aws-hosted-cp\n    spec:\n      template: aws-hosted-cp\n      config:\n        vpcID: vpc-0a000000000000000\n        region: us-west-1\n        publicIP: true\n        subnets:\n          - id: subnet-0aaaaaaaaaaaaaaaa\n            availabilityZone: us-west-1b\n        amiID: ami-0bfffffffffffffff\n        instanceType: t3.medium\n        securityGroupIDs:\n          - sg-0e000000000000000\n</code></pre> <p>Note</p> <p> In this example we're using the <code>us-west-1</code> region, but you should use the region of your VPC.</p>"},{"location":"aws/hosted-control-plane/#hmc-managedcluster-manifest-generation","title":"HMC ManagedCluster manifest generation","text":"<p>Grab the following <code>ManagedCluster</code> manifest template and save it to a file named <code>managedcluster.yaml.tpl</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp\n  config:\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n      - id: \"{{(index .spec.network.subnets 0).resourceID}}\"\n        availabilityZone: \"{{(index .spec.network.subnets 0).availabilityZone}}\"\n    amiID: ami-0bf2d31c356e4cb25\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Then run the following command to create the <code>managedcluster.yaml</code>:</p> <pre><code>kubectl get awscluster cluster -o go-template=\"$(cat managedcluster.yaml.tpl)\" &gt; managedcluster.yaml\n</code></pre>"},{"location":"aws/hosted-control-plane/#deployment-tips","title":"Deployment Tips","text":"<ul> <li>Ensure HMC templates and the controller image are somewhere public and   fetchable.</li> <li>For installing the HMC charts and templates from a custom repository, load   the <code>kubeconfig</code> from the cluster and run the commands:</li> </ul> <p><pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/mirantis/hmc/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/mirantis/hmc/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> * The infrastructure will need to manually be marked <code>Ready</code> to get the   <code>MachineDeployment</code> to scale up.  You can patch the <code>AWSCluster</code> kind using   the command:</p> <pre><code>KUBECONFIG=kubeconfig kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}' -n hmc-system\n</code></pre> <p>For additional information on why this is required click here.</p>"},{"location":"aws/main/","title":"Prepare the AWS infrastructure provider","text":""},{"location":"aws/main/#software-prerequisites","title":"Software prerequisites","text":"<ol> <li><code>kubectl</code> CLI installed locally.</li> <li><code>clusterawsadm</code> CLI installed locally.</li> </ol>"},{"location":"aws/main/#configure-aws-iam","title":"Configure AWS IAM","text":"<p>Follow the AWS IAM setup guide.</p>"},{"location":"aws/main/#aws-cluster-parameters","title":"AWS cluster parameters","text":"<p>Follow the AWS Cluster Parameters guide.</p>"},{"location":"aws/nuke/","title":"Nuking AWS resources","text":"<p>If you'd like to forcefully cleanup all AWS resources created by HMC you can use the following command:</p> <pre><code>CLUSTER_NAME=&lt;deployment-name&gt; make dev-aws-nuke\n</code></pre>"},{"location":"azure/cluster-parameters/","title":"Azure cluster parameters","text":""},{"location":"azure/cluster-parameters/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure CLI installed</li> <li><code>az login</code> command executed</li> </ul>"},{"location":"azure/cluster-parameters/#cluster-identity","title":"Cluster Identity","text":"<p>To provide credentials for CAPI Azure provider (CAPZ) the <code>AzureClusterIdentity</code> resource must be created. This should be done before provisioning any clusters.</p> <p>To create the <code>AzureClusterIdentity</code> you should first get the desired <code>SubscriptionID</code> by executing <code>az account list -o table</code> which will return list of subscriptions available to user.</p> <p>Then you need to create service principal which will be used by CAPZ to interact with Azure API. To do so you need to execute the following command:</p> <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;Subscription ID&gt;\"\n</code></pre> <p>The command will return json with the credentials for the service principal which will look like this:</p> <pre><code>{\n    \"appId\": \"29a3a125-7848-4ce6-9be9-a4b3eecca0ff\",\n    \"displayName\": \"azure-cli\",\n    \"password\": \"u_RANDOMHASH\",\n    \"tenant\": \"2f10bc28-959b-481f-b094-eb043a87570a\",\n}\n</code></pre> <p>Note</p> <p> Make sure to save this credentials and treat them like passwords.</p> <p>With the data from the json you can now create the <code>AzureClusterIdentity</code> object and it's secret.</p> <p>The objects created with the data above can look something like this:</p> <p>Secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: az-cluster-identity-secret\n  namespace: hmc-system\nstringData:\n  clientSecret: u_RANDOMHASH\ntype: Opaque\n</code></pre> <p>AzureClusterIdentity:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: az-cluster-identity\n  namespace: hmc-system\nspec:\n  allowedNamespaces: {}\n  clientID: 29a3a125-7848-4ce6-9be9-a4b3eecca0ff\n  clientSecret:\n    name: az-cluster-identity-secret\n    namespace: hmc-system\n  tenantID: 2f10bc28-959b-481f-b094-eb043a87570a\n  type: ServicePrincipal\n</code></pre> <p>Subscription ID which was used to create service principal should be the same that will be used in the <code>.spec.config.subscriptionID</code> field of the <code>ManagedCluster</code> object.</p> <p>To use <code>AzureClusterIdentity</code> it should be referenced in the <code>Credential</code> object. For more details check the credential section.</p>"},{"location":"azure/hosted-control-plane/","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"azure/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on Azure with HMC installed     on it</li> <li>Default storage class configured on the management cluster</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"azure/hosted-control-plane/#pre-existing-resources","title":"Pre-existing resources","text":"<p>Certain resources will not be created automatically in a hosted control plane scenario thus they should be created in advance and provided in the <code>ManagedCluster</code> object. You can reuse these resources with management cluster as described below.</p> <p>If you deployed your Azure Kubernetes cluster using Cluster API Provider Azure (CAPZ) you can obtain all the necessary data with the commands below:</p> <p>Location</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{.spec.location}}'\n</code></pre> <p>Subscription ID</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre> <p>Resource group</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre> <p>vnet name</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre> <p>Subnet name</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre> <p>Route table name</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre> <p>Security group name</p> <pre><code>kubectl get azurecluster &lt;cluster name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre>"},{"location":"azure/hosted-control-plane/#hmc-managedcluster-manifest","title":"HMC ManagedCluster manifest","text":"<p>With all the collected data your <code>ManagedCluster</code> manifest will look similar to this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp\n  config:\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    clusterIdentity:\n      name: az-cluster-identity\n      namespace: hmc-system\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n    tenantID: 7db9e0f2-c88a-4116-a373-9c8b6cc9d5eb\n    clientID: 471f65fa-ddee-40b4-90ae-da1a8a114ee1\n    clientSecret: \"u_RANDOM\"\n</code></pre> <p>To simplify creation of the ManagedCluster object you can use the template below:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp\n  config:\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    clusterIdentity:\n      name: az-cluster-identity\n      namespace: hmc-system\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n    tenantID: 7db9e0f2-c88a-4116-a373-9c8b6cc9d5eb\n    clientID: 471f65fa-ddee-40b4-90ae-da1a8a114ee1\n    clientSecret: \"u_RANDOM\"\n</code></pre> <p>Then you can render it using the command:</p> <pre><code>kubectl get azurecluster &lt;management cluster name&gt; -o go-template=\"$(cat template.yaml)\"\n</code></pre>"},{"location":"azure/hosted-control-plane/#cluster-creation","title":"Cluster creation","text":"<p>After applying <code>ManagedCluster</code> object you require to manually set the status of the <code>AzureCluster</code> object due to current limitations (see k0sproject/k0smotron#668).</p> <p>To do so you need to execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster name&gt; --type=merge --subresource status --patch 'status: {ready: true}'\n</code></pre>"},{"location":"azure/hosted-control-plane/#important-notes-on-the-cluster-deletion","title":"Important notes on the cluster deletion","text":"<p>Because of the aforementioned limitation you also need to make manual steps in order to properly delete cluster.</p> <p>Before removing the cluster make sure to place custom finalizer onto <code>AzureCluster</code> object. This is needed to prevent it from being deleted instantly which will cause cluster deletion to stuck indefinitely.</p> <p>To place finalizer you can execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster name&gt; --type=merge --patch 'metadata: {finalizers: [manual]}'\n</code></pre> <p>When finalizer is placed you can remove the <code>ManagedCluster</code> as usual. Check that all <code>AzureMachines</code> objects are deleted successfully and remove finalizer you've placed to finish cluster deletion.</p> <p>In case if have orphaned <code>AzureMachines</code> left you have to delete finalizers on them manually after making sure that no VMs are present in Azure.</p> <p>Note</p> <p> Since Azure admission prohibits orphaned objects mutation you'll have to disable it by deleting it's <code>mutatingwebhookconfiguration</code></p>"},{"location":"azure/machine-parameters/","title":"Azure machine parameters","text":""},{"location":"azure/machine-parameters/#ssh","title":"SSH","text":"<p>SSH public key can be passed to <code>.spec.config.sshPublicKey</code> (in case of hosted CP) parameter or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in case of standalone CP) of the <code>ManagedCluster</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"azure/machine-parameters/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"azure/machine-parameters/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume which defined in your image.</p>"},{"location":"azure/machine-parameters/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for you machine using the following parameters:</p> <ul> <li><code>.spec.config.image</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</li> </ul> <p>There are multiple self-excluding ways to define the image source (e.g. Azure Compute Gallery, Azure Marketplace, etc.).</p> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"azure/main/","title":"Installation methods","text":"<p>To begin, choose one of the following guides based on your setup:</p> <ul> <li> <p>Quick Start Guide   Follow the Quick Start Guide to rapidly deploy your Kubernetes clusters with minimal configuration.</p> </li> <li> <p>Hosted Control Plane (K0smotron) Guide   Refer to the Hosted Control Plane (K0smotron) Guide for detailed instructions on setting up a hosted control plane.</p> </li> </ul>"},{"location":"azure/quick-start/","title":"Quick Start","text":""},{"location":"azure/quick-start/#prerequisites","title":"Prerequisites","text":""},{"location":"azure/quick-start/#required-tools","title":"Required Tools","text":"<p>Before we begin, deploying Kubernetes clusters on Azure using Project 2A, make sure you have:</p> <ol> <li> <p>Azure CLI (<code>az</code>):    The <code>az</code> CLI is required to interact with Azure resources. Install it by    following the    Azure CLI installation instructions.</p> <p>Run the <code>az login</code> command to authenticate your session with Azure.</p> </li> <li> <p><code>kubectl</code>:    Make sure <code>kubectl</code> is installed on your local machine to manage your    Kubernetes clusters.</p> <p>You can follow the official installation guide.</p> </li> </ol>"},{"location":"azure/quick-start/#azure-account-access","title":"Azure account access","text":"<p>Ensure you have access to an Azure account with the necessary permissions to manage resources. You will need to register specific resource providers, which are listed below.</p>"},{"location":"azure/quick-start/#register-resource-providers","title":"Register resource providers","text":"<p>If you're using a new subscription, register these services ensure the following resource providers are registered:</p> <ul> <li><code>Microsoft.Compute</code></li> <li><code>Microsoft.Network</code></li> <li><code>Microsoft.ContainerService</code></li> <li><code>Microsoft.ManagedIdentity</code></li> <li><code>Microsoft.Authorization</code></li> </ul> <p>To register these providers, you can run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> <p>You can follow the official documentation guide to register the providers.</p>"},{"location":"azure/quick-start/#setup-the-azure-environment","title":"Setup the Azure Environment","text":"<p>Before you can create a cluster on Azure, you need to set up credentials. This involves creating an <code>AzureClusterIdentity</code> and a <code>Service Principal (SP)</code> to let CAPZ (Cluster API Azure) communicate with Azure.</p>"},{"location":"azure/quick-start/#step-1-find-your-subscription-id","title":"Step 1: Find Your Subscription ID","text":"<p>Open your terminal and log into your Azure account:</p> <pre><code>az login\n</code></pre> <p>List all your Azure subscriptions:</p> <pre><code>az account list -o table\n</code></pre> <p>Look for the Subscription ID of the account you want to use.</p> <p>Example output:</p> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    12345678-1234-5678-1234-567812345678  87654321-1234-5678-1234-12345678\n</code></pre> <p>Copy your chosen Subscription ID for the next step.</p>"},{"location":"azure/quick-start/#step-2-create-a-service-principal-sp","title":"Step 2: Create a Service Principal (SP)","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure.</p> <p>In your terminal, run the following command. Make sure to replace  with the ID you copied earlier: <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;Subscription ID&gt;\"\n</code></pre> <p>You will see output like this:</p> <pre><code>{\n \"appId\": \"12345678-7848-4ce6-9be9-a4b3eecca0ff\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"12~34~I5zKrL5Kem2aXsXUw6tIig0M~3~1234567\",\n \"tenant\": \"12345678-959b-481f-b094-eb043a87570a\",\n}\n</code></pre> <p>Copy and save these values somewhere safe, for creating the <code>AzureClusterIdentity</code> object and it's secret:</p> <pre><code>appId -&gt; This is the Client ID\npassword -&gt; This is the Client Secret\ntenant -&gt; This is the Tenant ID\n</code></pre> <p>Subscription ID which was used to create service principal should be the same that will be used in the <code>.spec.config.subscriptionID</code> field of the <code>ManagedCluster</code> object.</p> <p>To use <code>AzureClusterIdentity</code> it should be referenced in the <code>Credential</code> object. For more details check the credential section.</p> <p>Note</p> <p> Make sure to save this credentials and treat them like passwords.</p> <p>Now that you have your <code>Subscription ID</code>, <code>Client ID</code>, <code>Client Secret</code>, and <code>Tenant ID</code>, you can create the AzureClusterIdentity resource, which will use these credentials to manage your cluster on Azure.</p>"},{"location":"azure/quick-start/#create-the-azure-clusteridentity-resources","title":"Create the Azure ClusterIdentity resources","text":""},{"location":"azure/quick-start/#step-1-create-the-secret-object","title":"Step 1: Create the Secret Object","text":"<p>The Secret stores the clientSecret (password) from the Service Principal.</p> <ol> <li>Save the Secret YAML into a file named <code>az-cluster-identity-secret.yaml</code>:</li> </ol> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: az-cluster-identity-secret\nnamespace: hmc-system\nstringData:\nclientSecret: password # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f az-cluster-identity-secret.yaml\n</code></pre>"},{"location":"azure/quick-start/#step-2-create-the-azureclusteridentity-object","title":"Step 2: Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials CAPZ will use to manage Azure resources.</p> <ol> <li>Save the AzureClusterIdentity YAML into a file named <code>az-cluster-identity.yaml</code>:</li> </ol> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\nlabels:\n   clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\nname: az-cluster-identity\nnamespace: hmc-system\nspec:\nallowedNamespaces: {}\nclientID: appId # AppId retrieved from the Service Principal\nclientSecret:\n   name: az-cluster-identity-secret\n   namespace: hmc-system\ntenantID: tenant # TennantID retrieved from the Service Principal\ntype: ServicePrincipal\n</code></pre> <ol> <li>Apply the YAML to your cluster:</li> </ol> <pre><code>   kubectl apply -f az-cluster-identity.yaml\n</code></pre>"},{"location":"azure/quick-start/#step-3-verify-the-resources","title":"Step 3: Verify the Resources","text":"<p>After applying both YAML files, you can verify that the objects were created successfully:</p> <ol> <li>Check the Secret:</li> </ol> <pre><code>kubectl get secret az-cluster-identity-secret -n hmc-system\n</code></pre> <ol> <li>Check the AzureClusterIdentity:</li> </ol> <pre><code>kubectl get azureclusteridentity az-cluster-identity -n hmc-system\n</code></pre>"},{"location":"azure/quick-start/#install-hmc","title":"Install HMC","text":"<p>To install HMC, first check the latest release at https://github.com/Mirantis/hmc/tags.</p> <p>Then, run the following command to deploy HMC on your cluster:</p> <pre><code>kubectl apply -f https://github.com/Mirantis/hmc/releases/download/v0.0.3/install.yaml\n</code></pre> <p>Wait a moment for the pods to initialize. Once complete, you should see output similar to the following:</p> <p><pre><code>kubectl get po -n hmc-system\nNAMESPACE                           NAME                                                             READY   STATUS              RESTARTS        AGE\ncert-manager                        cert-manager-6bcd5c585d-tdtx7                                    1/1     Running             2               14m\ncert-manager                        cert-manager-cainjector-df6db5846-6npqg                          1/1     Running             1 (3m31s ago)   14m\ncert-manager                        cert-manager-webhook-6ff7fbb9ff-nvfl9                            1/1     Running             2               14m\nhmc-system                          helm-controller-76f675f6b7-qwvxv                                 1/1     Running             4               54m\nhmc-system                          hmc-cert-manager-7c8bd964b4-l4pls                                1/1     Running             2 (4m1s ago)    54m\nhmc-system                          hmc-cert-manager-cainjector-56476c46f9-fx2sk                     1/1     Running             0               54m\nhmc-system                          hmc-cert-manager-webhook-69d7fccf68-nzzvq                        1/1     Running             2               54m\nhmc-system                          hmc-controller-manager-855fbf9586-f44ch                          1/1     Running             5 (2m44s ago)   54m\nhmc-system                          hmc-flux-check-svfzl                                             0/1     Completed           0               54m\nhmc-system                          source-controller-5f648d6f5d-fkfxh                               1/1     Running             4 (2m44s ago)   54m\n</code></pre> Once all the components are installed, check the clustertemplate with:</p> <pre><code>kubectl get clustertemplate -n hmc-system\nNAME                          VALID\naws-eks-0-0-1                 false\naws-hosted-cp-0-0-2           false\naws-standalone-cp-0-0-2       false\nazure-hosted-cp-0-0-2         false\nazure-standalone-cp-0-0-2     false\nvsphere-hosted-cp-0-0-2       false\nvsphere-standalone-cp-0-0-2   false\n</code></pre>"},{"location":"azure/quick-start/#enable-the-cluster-template-by","title":"Enable the cluster template by","text":""},{"location":"azure/quick-start/#step-1","title":"Step 1","text":""},{"location":"azure/quick-start/#step-2","title":"Step 2","text":""},{"location":"azure/quick-start/#create-the-management-server","title":"Create the Management Server","text":""},{"location":"azure/quick-start/#test-management-cluster-statushealth","title":"Test Management cluster status/health","text":""},{"location":"azure/quick-start/#create-workload-clusters","title":"Create workload clusters","text":""},{"location":"azure/quick-start/#accessing-workload-cluster","title":"Accessing workload cluster","text":""},{"location":"azure/quick-start/#deleting-everything","title":"Deleting everything","text":""},{"location":"azure/quick-start/#azure-machine-parameters","title":"Azure machine parameters","text":"<p>Follow the Azure machine parameters guide if you want to setup/modify the default machine parameters.</p>"},{"location":"azure/quick-start/#hosted-control-plane","title":"Hosted Control Plane","text":"<p>For more advanced setups, follow the Hosted Control Plane (K0smotron) Guide to configure a hosted control plane. </p> <p>Note</p> <p> In this setup, all control plane components for managed clusters will reside within the management cluster, offering centralized control and easier management.</p>"},{"location":"credential/main/","title":"Credential system","text":"<p>In order for infrastructure provider to work properly a correct credentials should be passed to it. The following describes how it is implemented in Project 0x2A.</p>"},{"location":"credential/main/#the-process","title":"The process","text":"<p>The following is the process of passing credentials to the system:</p> <ol> <li>Provider specific <code>ClusterIdentity</code> and <code>Secret</code> are created</li> <li><code>Credential</code> object is created referencing <code>ClusterIdentity</code> from step 1.</li> <li>The <code>Credential</code> object is then referenced in the <code>ManagedCluster</code>.</li> </ol> <p>By design steps 1 and 2 should be executed by the platform lead engineer who has access to the credentials. Thus credentials could be used by platform engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIndentity</code>.</p>"},{"location":"credential/main/#credential-object","title":"Credential object","text":"<p>The <code>Credential</code> object acts like a reference to the underlying credentials. It is namespace-scoped, which means that it must be in the same <code>Namespace</code> with the <code>ManagedCluster</code> it is referenced in. Actual credentials can be located in any namespace.</p>"},{"location":"credential/main/#example","title":"Example","text":"<pre><code>---\napiVersion: hmc.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-credential\n  namespace: dev\nspec:\n  description: \"Main Azure credentials\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: hmc-system\n</code></pre> <p>In the example above <code>Credential</code> object is referencing <code>AzureClusterIdentity</code> which was created in the <code>hmc-system</code> namespace.</p> <p>The <code>.spec.description</code> field can be used to provide arbitrary description of the object, so user could make a decision which credentials to use if several are present.</p>"},{"location":"credential/main/#cloud-provider-credentials-propagation","title":"Cloud provider credentials propagation","text":"<p>Some components in the managed cluster require cloud provider credentials to be passed for proper functioning. As an example cloud controller manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge of credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials. This approach has several problems:</p> <ul> <li>Credentials stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in Project 2A we're using special controller which aggregates all necessary data from CAPI provider resources (like <code>ClusterIdentity</code>) and creates <code>Secrets</code> directly on the managed cluster.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also this automation makes it possible to separate roles and responsibilities where only platform lead has access to credentials and platform engineer can only use them without seeing concrete values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within <code>ManagedCluster</code> reconcile process, user only needs to provide correct Credential object.</p>"},{"location":"credential/main/#provider-specific-notes","title":"Provider specific notes","text":"<p>Since this feature highly depends on the provider some notes and clarifications are needed.</p> Research notes <p> More detailed research notes could be found here</p>"},{"location":"credential/main/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p>"},{"location":"credential/main/#azure","title":"Azure","text":"<p>Currently Cluster API provider Azure (CAPZ) creates <code>azure.json</code> Secrets in the same namespace with <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> yaml later during bootstrap process.</p> <p>In Project 2A these Secrets aren't used and will not be added to the <code>cloud-init</code>, but platform engineer can access them unrestricted.</p>"},{"location":"install/installation/","title":"Installation Guide","text":""},{"location":"install/installation/#installation","title":"Installation","text":"<pre><code>export KUBECONFIG=&lt;path-to-management-kubeconfig&gt;\n</code></pre> <pre><code>helm install hmc oci://ghcr.io/mirantis/hmc/charts/hmc --version &lt;hmc-version&gt; -n hmc-system --create-namespace\n</code></pre>"},{"location":"install/installation/#extended-management-configuration","title":"Extended Management configuration","text":"<p>By default, the Hybrid Container Cloud is being deployed with the following configuration:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: hmc\nspec:\n  core:\n    capi:\n      template: cluster-api\n    hmc:\n      template: hmc\n  providers:\n  - template: k0smotron\n  - config:\n      configSecret:\n       name: aws-variables\n    template: cluster-api-provider-aws\n</code></pre> <p>There are two options to override the default management configuration of HMC:</p> <ol> <li> <p>Update the <code>Management</code> object after the HMC installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy HMC skipping the default <code>Management</code> object creation and provide your own <code>Management</code> configuration:</p> </li> <li> <p>Create <code>management.yaml</code> file and configure core components and providers.    See Management API.</p> </li> <li> <p>Specify <code>--create-management=false</code> controller argument and install HMC:</p> <p>If installing using <code>helm</code> add the following parameter to the <code>helm install</code> command:</p> <p><code>--set=\"controller.createManagement=false\"</code></p> </li> <li> <p>Create <code>hmc</code> <code>Management</code> object after HMC installation:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml</code></p> </li> </ol>"},{"location":"install/installation/#deploy-a-managed-cluster","title":"Deploy a managed cluster","text":"<p>To deploy a managed cluster:</p> <ol> <li>Select the <code>Template</code> you want to use for the deployment. To list all available templates, run:</li> </ol> <pre><code>export KUBECONFIG=&lt;path-to-management-kubeconfig&gt;\n\nkubectl get template -n hmc-system -o go-template='{{ range .items }}{{ if eq .status.type \"deployment\" }}{{ .metadata.name }}{{ printf \"\\n\" }}{{ end }}{{ end }}'\n</code></pre> <p>For details about the <code>Template system</code> in HMC, see Templates system.</p> <p>If you want to deploy hostded control plate template, make sure to check additional notes on Hosted control plane.</p> <ol> <li>Create the file with the <code>ManagedCluster</code> configuration:</li> </ol> <p>Note</p> <p> Substitute the parameters enclosed in angle brackets with the corresponding values. Enable the <code>dryRun</code> flag if required. For details, see Dry run.</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;cluster-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  dryRun: &lt;true/false&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <ol> <li>Create the <code>ManagedCluster</code> object:</li> </ol> <p><code>kubectl create -f managedcluster.yaml</code></p> <ol> <li>Check the status of the newly created <code>ManagedCluster</code> object:</li> </ol> <p><code>kubectl -n &lt;managedcluster-namespace&gt; get managedcluster.hmc &lt;managedcluster-name&gt; -o=yaml</code></p> <ol> <li>Wait for infrastructure to be provisioned and the cluster to be deployed (the provisioning starts only when <code>spec.dryRun</code> is disabled):</li> </ol> <p><code>kubectl -n &lt;managedcluster-namespace&gt; get cluster &lt;managedcluster-name&gt; -o=yaml</code></p> <p>Tip</p> <p> You may also watch the process with the <code>clusterctl describe</code> command (requires the <code>clusterctl</code> CLI to be installed): <pre><code>clusterctl describe cluster &lt;managedcluster-name&gt; -n &lt;managedcluster-namespace&gt; --show-conditions all\n</code></pre></p> <ol> <li>Retrieve the <code>kubeconfig</code> of your managed cluster:</li> </ol> <pre><code>kubectl get secret -n hmc-system &lt;managedcluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre>"},{"location":"install/installation/#dry-run","title":"Dry run","text":"<p>HMC <code>ManagedCluster</code> supports two modes: with and without (default) <code>dryRun</code>.</p> <p>If no configuration (<code>spec.config</code>) provided, the <code>ManagedCluster</code> object will be populated with defaults (default configuration can be found in the corresponding <code>Template</code> status) and automatically marked as <code>dryRun</code>.</p> <p>Here is an example of the <code>ManagedCluster</code> object with default configuration:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;cluster-namespace&gt;\nspec:\n  config:\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    controlPlaneNumber: 3\n    k0s:\n      version: v1.27.2+k0s.0\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    workersNumber: 2\n  template: aws-standalone-cp\n  dryRun: true\n</code></pre> <p>After you adjust your configuration and ensure that it passes validation (<code>TemplateReady</code> condition from <code>status.conditions</code>), remove the <code>spec.dryRun</code> flag to proceed with the deployment.</p> <p>Here is an example of a <code>ManagedCluster</code> object that passed the validation:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: aws-standalone\n  namespace: aws\nspec:\n  template: aws-standalone-cp\n  config:\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      amiID: ami-02f3416038bdb17fb\n      instanceType: t3.small\n    worker:\n      amiID: ami-02f3416038bdb17fb\n      instanceType: t3.small\n  status:\n    conditions:\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: Template is valid\n      reason: Succeeded\n      status: \"True\"\n      type: TemplateReady\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: Helm chart is valid\n      reason: Succeeded\n      status: \"True\"\n      type: HelmChartReady\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: ManagedCluster is ready\n      reason: Succeeded\n      status: \"True\"\n      type: Ready\n    observedGeneration: 1\n</code></pre>"},{"location":"install/installation/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the Management object:</li> </ol> <p><code>kubectl delete management.hmc hmc</code></p> <p>Note</p> <p> Make sure you have no HMC ManagedCluster objects left in the cluster prior to Management deletion</p> <ol> <li>Remove the <code>hmc</code> Helm release:</li> </ol> <p><code>helm uninstall hmc -n hmc-system</code></p> <ol> <li>Remove the <code>hmc-system</code> namespace:</li> </ol> <p><code>kubectl delete ns hmc-system</code></p>"},{"location":"install/quick-start/","title":"Quick Start","text":""},{"location":"install/quick-start/#tldr","title":"TL;DR","text":"<pre><code>kubectl apply -f https://github.com/Mirantis/hmc/releases/download/v0.0.1/install.yaml\n</code></pre> <p>or install using <code>helm</code></p> <pre><code>helm install hmc oci://ghcr.io/mirantis/hmc/charts/hmc --version v0.0.1 -n hmc-system --create-namespace\n</code></pre> <p>Note</p> <p> The HMC installation using Kubernetes manifests does not allow customization of the deployment. If the custom HMC configuration should be applied, install HMC using the Helm chart.</p>"},{"location":"template/main/","title":"Templates system","text":"<p>By default, Hybrid Container Cloud delivers a set of default <code>Template</code> objects. You can also build your own templates and use them for deployment.</p>"},{"location":"template/main/#custom-deployment-templates","title":"Custom deployment Templates","text":"<p>At the moment all <code>Templates</code> should reside in the <code>hmc-system</code> namespace. But they can be referenced by <code>ManagedClusters</code> from any namespace.</p> <p>Here are the instructions on how to bring your own Template to HMC:</p> <ol> <li>Create a HelmRepository object containing the URL to the external Helm repository. Label it with <code>hmc.mirantis.com/managed: \"true\"</code>.</li> <li>Create a HelmChart object referencing the <code>HelmRepository</code> as a <code>sourceRef</code>, specifying the name and version of your Helm chart. Label it with <code>hmc.mirantis.com/managed: \"true\"</code>.</li> <li>Create a <code>Template</code> object in <code>hmc-system</code> namespace referencing this helm chart in <code>spec.helm.chartRef</code>. <code>chartRef</code> is a field of the CrossNamespaceSourceReference kind.</li> </ol> <p>Here is an example of a custom <code>Template</code> with the <code>HelmChart</code> reference:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: hmc-system\n  labels:\n    hmc.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmChart\nmetadata:\n  name: custom-template-chart\n  namespace: hmc-system\n  labels:\n    hmc.mirantis.com/managed: \"true\"\nspec:\n  interval: 5m0s\n  chart: custom-template-chart-name\n  reconcileStrategy: ChartVersion\n  sourceRef:\n    kind: HelmRepository\n    name: custom-templates-repo\n  version: 0.2.0\n</code></pre> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: Template\nmetadata:\n  name: os-k0smotron\n  namespace: hmc-system\nspec:\n  type: deployment\n  providers:\n    infrastructure:\n      - openstack\n    bootstrap:\n      - k0s\n    controlPlane:\n      - k0smotron\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: custom-template-chart\n      namespace: default\n</code></pre> <p>The <code>Template</code> should follow the rules mentioned below:</p> <ol> <li><code>spec.type</code> should be <code>deployment</code> (as an alternative, the referenced helm chart may contain the <code>hmc.mirantis.com/type: deployment</code> annotation in <code>Chart.yaml</code>).</li> <li> <p><code>spec.providers</code> should contain the list of required Cluster API providers: <code>infrastructure</code>, <code>bootstrap</code> and <code>controlPlane</code>. As an alternative, the referenced helm chart may contain the specific annotations in the <code>Chart.yaml</code> (value is a list of providers divided by semicolon). These fields are only used for validation. For example:</p> <p><code>Template</code> spec:</p> <pre><code>spec:\n  providers:\n    infrastructure:\n      - aws\n    bootstrap:\n      - k0s\n    controlPlane:\n      - k0smotron\n</code></pre> <p><code>Chart.yaml</code>:</p> <pre><code>annotations:\n  hmc.mirantis.com/infrastructure-providers: aws\n  hmc.mirantis.com/control-plane-providers: k0s; k0smotron\n  hmc.mirantis.com/bootstrap-providers: k0s\n</code></pre> </li> </ol>"},{"location":"template/main/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes, including exact versions or version constraints. Both must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (version and constraint, (e.g. <code>k8sVersion</code> and <code>k8sConstraint</code>)) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set an exact compatible <code>CAPI</code> version along with the core <code>CAPI</code> version constraints, and exact compatibility versions for each of the <code>infrastructure</code>, <code>bootstrap</code> and <code>controlPlane</code> providers type. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>The exact <code>CAPI</code> version must be only set for the core <code>CAPI</code> <code>ProviderTemplate</code>, and the <code>CAPI</code> constraint must be set for other <code>ProviderTemplate</code> objects. The attributes are mutually exclusive.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ProviderTemplate\n# ...\nspec:\n  # capiVersion makes sense only for the core CAPI template\n  # capiVersion: 1.7.3 # only exact version is applicable\n  capiVersionConstraint: ~1.7.0 # only version constraints are applicable\n  providers:\n    bootstrap:\n    - name: k0s\n      versionOrConstraint: 1.0.0 # only exact version is applicable\n    controlPlane:\n    - name: k0smotron\n      versionOrConstraint: 1.34.0 # only exact version is applicable\n    infrastructure:\n    - name: aws\n      versionOrConstraint: 1.2.3 # only exact version is applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code># hmc.mirantis.com/capi-version makes sense only for the core CAPI template\n# hmc.mirantis.com/capi-version: 1.7.3\nhmc.mirantis.com/capi-version-constraint: ~1.7.0\nhmc.mirantis.com/bootstrap-providers: k0s 1.0.0 # space-separated semicolon-separated list, e.g. k0s 1.0.0; k0s 1.0.1\nhmc.mirantis.com/control-plane-providers: k0smotron 1.34.0 # space-separated semicolon-separated list, e.g. k0s 1.0.0; k0smotron 1.34.5\nhmc.mirantis.com/infrastructure-providers: aws 1.2.3 # space-separated semicolon-separated list\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version along with compatibility constrained versions for each of the <code>infrastructure</code>, <code>bootstrap</code> and <code>controlPlane</code> providers type to match against the related <code>ProviderTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>spec</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact version is applicable\n  providers:\n    bootstrap:\n    - name: k0s\n      versionOrConstraint: \"&gt;=1.0.0\" # only version constraints are applicable\n    controlPlane:\n    - name: k0smotron\n      versionOrConstraint: \"&gt;=1.34.0 &lt;2.0.0-0\" # only version constraints are applicable\n    infrastructure:\n    - name: aws\n      versionOrConstraint: \"~1.2.3\" # only version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>hmc.mirantis.com/k8s-version: 1.30.0\nhmc.mirantis.com/bootstrap-providers: k0s &gt;=1.0.0 # space-separated semicolon-separated list\nhmc.mirantis.com/control-plane-providers: k0smotron &gt;=1.34.0 &lt;2.0.0-0 # space-separated semicolon-separated list, e.g. k0s &gt;=1.0.0; k0smotron ~1.34.0\nhmc.mirantis.com/infrastructure-providers: aws ~1.2.3 # space-separated semicolon-separated list\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>spec</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>hmc.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"template/main/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are being checked sticking to the following rules:</p> <ul> <li>both the exact and constraint version of the same type (e.g. <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set otherwise no check is performed;</li> <li>if a <code>ProviderTemplate</code> object's exact providers versions do not satisfy the providers' versions constraints from the related <code>ClusterTemplate</code> object, the updates to the <code>ManagedCluster</code> object will be blocked;</li> <li>if the core <code>CAPI</code> <code>ProviderTemplate</code> exact <code>CAPI</code> version does no satisfy the <code>CAPI</code> version constraints from other of <code>ProviderTemplate</code> objects, the to the <code>Management</code> object will be blocked;</li> <li>if a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ManagedCluster</code> object will be blocked.</li> </ul>"},{"location":"template/main/#remove-templates-shipped-with-hmc","title":"Remove Templates shipped with HMC","text":"<p>If you need to limit the cluster templates that exist in your HMC installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>deployment</code> Templates shipped with HMC:</p> <pre><code>kubectl get templates -n hmc-system -l helm.toolkit.fluxcd.io/name=hmc-templates  | grep deployment\n</code></pre> <p>Example output:</p> <pre><code>aws-hosted-cp              deployment   true\naws-standalone-cp          deployment   true\n</code></pre> </li> <li> <p>Remove the templates from the list:</p> <pre><code>kubectl delete template -n hmc-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"template/template-management/","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>TemplateManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. <code>TemplateManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition to deliver templates into and the template chains. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of the Cluster Template Management:</p> <ol> <li>Create <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>hmc-system</code>). Properly configure    the list of <code>availableUpgrades</code> for the specified <code>ClusterTemplate</code> if the upgrade is allowed. For example:</li> </ol> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: hmc-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-1\n      availableUpgrades:\n        - name: aws-standalone-cp-0-0-2\n    - name: aws-standalone-cp-0-0-2\n</code></pre> <ol> <li>Edit <code>TemplateManagement</code> object and configure the <code>spec.accessRules</code>.    For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the    <code>default</code> namespace, the following <code>accessRule</code> should be added:</li> </ol> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> <p>The HMC controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-1</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code> (available for the upgrade from <code>aws-standalone-cp-0-0-1</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as the available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then, recreated from scratch even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol>"},{"location":"update/cluster-update/","title":"Managed Cluster update","text":"<p>To update the <code>ManagedCluster</code> update <code>spec.template</code> in the <code>ManagedCluster</code> object to the new <code>ClusterTemplate</code> name:</p> <p>Run:</p> <pre><code>kubectl patch managedcluster.hmc &lt;cluster-name&gt; -n &lt;cluster-namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> <p>Then, check the status of the <code>ManagedCluster</code> object:</p> <pre><code>kubectl get managedcluster.hmc &lt;cluster-name&gt; -n &lt;cluster-namespace&gt;\n</code></pre> <p>In the commands above, replace the parameters enclosed in angle brackets with the corresponding values.</p> <p>To get more details, run the previous command with <code>-o=yaml</code> option and check the <code>status.conditions</code>.</p> <p>Note</p> <p> The <code>ManagedCluster</code> is allowed to be updated to the specific templates only. The templates available for the update are defined in the <code>ClusterTemplateChain</code> objects. Also, the <code>TemplateManagement</code> object should contain properly configured <code>spec.accessRules</code> with the list of <code>ClusterTemplateChain</code> object names and the namespaces where the supported templates from the chain spec will be delivered. For details, see: Template Life Cycle Management</p>"},{"location":"vsphere/cluster-parameters/","title":"vSphere cluster parameters","text":""},{"location":"vsphere/cluster-parameters/#prerequisites","title":"Prerequisites","text":"<ul> <li>vSphere provider prerequisites are complete.</li> </ul>"},{"location":"vsphere/cluster-parameters/#cluster-identity","title":"Cluster Identity","text":"<p>To provide credentials for CAPI vSphere provider (CAPV) the <code>VSphereClusterIdentity</code> resource must be created. This should be done before provisioning any clusters.</p> <p>To create cluster identity you'll only need username and password for your vSphere instance.</p> <p>The example of the objects can be found below:</p> <p>Secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: hmc-system\nstringData:\n  username: user\n  password: Passw0rd\n</code></pre> <p>VsphereClusterIdentity:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>To be used for the cluster creation.<code>VsphereClusterIdentity</code> then should be referenced in the <code>Credential</code> object.</p> <p>For more details regarding the credential system check the credential section of the docs.</p>"},{"location":"vsphere/cluster-parameters/#managedcluster-parameters","title":"ManagedCluster parameters","text":"<p>To deploy managed cluster a number of parameters should be passed to the <code>ManagedCluster</code> object.</p>"},{"location":"vsphere/cluster-parameters/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters, which are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere server <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> vSphere folder path <p>You can check machine parameters for machine specific parameters.</p> <p>To obtain vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre>"},{"location":"vsphere/cluster-parameters/#example-of-managedcluster-cr","title":"Example of ManagedCluster CR","text":"<p>With all above parameters provided your <code>ManagedCluster</code> can look like this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp\n  credential: vsphere-credential\n  config:\n    clusterIdentity:\n      name: vsphere-cluster-identity\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre>"},{"location":"vsphere/hosted-control-plane/","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"vsphere/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on vSphere with HMC installed   on it</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"vsphere/hosted-control-plane/#managedcluster-manifest","title":"ManagedCluster manifest","text":"<p>Hosted CP template has mostly identical parameters with standalone CP, you can check them in the cluster parameters and the machine parameters sections.</p> <p>Important note on control plane endpoint IP</p> <p>Since vSphere provider requires that user will provide control plane endpoint IP before deploying the cluster you should make sure that this IP will be the same that will be assigned to the k0smotron LB service. Thus you must provide control plane endpoint IP to the k0smotron service via annotation which is accepted by your LB provider (in the following example <code>kube-vip</code> annotation is used)</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp\n  config:\n    clusterIdentity:\n      name: vsphere-cluster-identity\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n      username: \"user\"\n      password: \"Passw0rd\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre>"},{"location":"vsphere/machine-parameters/","title":"vSphere machine parameters","text":""},{"location":"vsphere/machine-parameters/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that user is already created during template creation. Because of that you must pass username along with SSH public key to configure SSH access.</p> <p>SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in case of hosted CP) parameter or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> parameters (in case of standalone CP) of the <code>ManagedCluster</code> object.</p> <p>SSH public key must be passed literally as a string.</p> <p>Username can be passed to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code> depending on you deployment model.</p>"},{"location":"vsphere/machine-parameters/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"vsphere/machine-parameters/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ManagedCluster</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"vsphere/main/","title":"vSphere provider","text":""},{"location":"vsphere/main/#prerequisites","title":"Prerequisites","text":"<ol> <li><code>kubectl</code> CLI installed locally.</li> <li>vSphere instance version <code>6.7.0</code> or higher.</li> <li>vSphere account with appropriate privileges.</li> <li>Image template.</li> <li>vSphere network with DHCP enabled.</li> </ol>"},{"location":"vsphere/main/#image-template","title":"Image template","text":"<p>You can use pre-buit image templates from CAPV project or build your own.</p> <p>When building your own image make sure that vmware tools and cloud-init are installed and properly configured.</p> <p>You can follow official open-vm-tools guide on how to correctly install vmware-tools.</p> <p>When setting up cloud-init you can refer to official docs and specifically vmware datasource docs for extended information regarding cloud-init on vSphere.</p>"},{"location":"vsphere/main/#vsphere-network","title":"vSphere network","text":"<p>When creating network make sure that it has DHCP service.</p> <p>Also make sure that the part of your network is out of DHCP range (e.g. network 172.16.0.0/24 with DHCP range 172.16.0.100-172.16.0.254). This is needed to make sure that LB services will not create any IP conflicts in the network.</p>"},{"location":"vsphere/main/#vsphere-privileges","title":"vSphere privileges","text":"<p>To function properly the user assigned to vSphere provider should be able to manipulate vSphere resources. The following is the general overview of the required privileges:</p> <ul> <li><code>Virtual machine</code> - full permissions are required</li> <li><code>Network</code> - <code>Assign network</code> is sufficient</li> <li><code>Datastore</code> - it should be possible for user to manipulate virtual machine   files and metadata</li> </ul> <p>In addition to that specific CSI driver permissions are required see the official doc to get more information on CSI specific permissions.</p>"},{"location":"vsphere/main/#vsphere-cluster-parameters","title":"vSphere cluster parameters","text":"<p>Follow the vSphere cluster parameters guide to setup mandatory parameters for vSphere clusters.</p>"},{"location":"vsphere/main/#vsphere-machine-parameters","title":"vSphere machine parameters","text":"<p>Follow the vSphere machine parameters guide if you want to setup/modify the default machine parameters.</p>"},{"location":"vsphere/main/#vsphere-hosted-control-plane","title":"vSphere hosted control plane","text":"<p>Follow the Hosted control plane guide to deploy hosted control plane cluster on vSphere.</p>"}]}